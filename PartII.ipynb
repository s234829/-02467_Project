{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a6705b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Friis\\anaconda3\\Lib\\site-packages\\tqdm_joblib\\__init__.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Friis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import gzip\n",
    "import json\n",
    "import ast\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Data manipulation and numerical operations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Parallel processing and progress tracking\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from tqdm_joblib import tqdm_joblib\n",
    "\n",
    "# Natural Language Processing\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Graph and network analysis\n",
    "import networkx as nx\n",
    "from networkx.readwrite import json_graph\n",
    "import netwulf as nw\n",
    "import community as community_louvain  # Louvain algorithm for community detection\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Set random seed for reproducibility (Louvain algorithm)\n",
    "np.random.seed(seed=100)  # Does it work???\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc14332",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "Motivation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5964b389",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e353696",
   "metadata": {},
   "source": [
    "Our dataset is a subset of the \"Amazon Reviews\" dataset collected in 2023 by McAuley Lab:\n",
    "\n",
    "Link to dataset: https://amazon-reviews-2023.github.io/\n",
    "\n",
    "We are not interested in the entire 571M+ reviews but will be looking specifically at the \"Movies_and_TV\" subset. Each subset is divided into two .csv files, a review and a metadata.\n",
    "\n",
    "The review contains - as the name would suggest - the reviews themselves. With zero cleaning, the combined dataset is around 7GB. Almost immediately there are large portions of the dataset we deem to not be relevant data and so we will be shrinking the usable dataset, to get a much cleaner and more potent dataset for the purposes of creating the co-reviewer graph. \n",
    "\n",
    "Firstly, an overview of the columns in each dataset:\n",
    "\n",
    "| Reviews             | Description                                                                 |\n",
    "|--------------------|-----------------------------------------------------------------------------|\n",
    "| `rating`           | Star rating given by the reviewer (e.g., 1 to 5)                            |\n",
    "| `title`            | Title or headline of the review                                             |\n",
    "| `text`             | The main body text of the review                                            |\n",
    "| `images`           | Image URLs or attachments included with the review (if any)                 |\n",
    "| `asin`             | Amazon Standard Identification Number for the specific product              |\n",
    "| `parent_asin`      | Group identifier to cluster product variants                                |\n",
    "| `user_id`          | Anonymized unique identifier for the reviewer                               |\n",
    "| `timestamp`        | Time the review was posted (UNIX or ISO format)                             |\n",
    "| `helpful_vote`     | Number of people who found the review helpful                               |\n",
    "| `verified_purchase`| Boolean indicating whether the purchase was verified by Amazon             |\n",
    "\n",
    "| Metadata           | Description                                                                 |\n",
    "|------------------|-----------------------------------------------------------------------------|\n",
    "| `main_category`  | The top-level category the product belongs to (e.g., \"Movies & TV\")         |\n",
    "| `title`          | The main product title (often the movie/show title)                         |\n",
    "| `subtitle`       | Optional subtitle for the product (e.g., edition/version info)              |\n",
    "| `average_rating` | The average customer rating for the product                                 |\n",
    "| `rating_number`  | Total number of ratings the product has received                            |\n",
    "| `features`       | A list of product features (e.g., language, format)                         |\n",
    "| `description`    | A longer description or summary of the product                              |\n",
    "| `price`          | The listed price of the product                                              |\n",
    "| `images`         | A list of image URLs associated with the product                            |\n",
    "| `videos`         | Video media links (e.g., trailers or previews), if available                |\n",
    "| `store`          | The Amazon store/subcategory under which the product is listed              |\n",
    "| `categories`     | List of categories/tags assigned to the product (e.g., genre, theme)        |\n",
    "| `details`        | Additional technical or marketing metadata (format, region, etc.)           |\n",
    "| `parent_asin`    | A group-level identifier for versions of the same product                   |\n",
    "\n",
    "\n",
    "A couple of these drastically reduce the size like, `helpful_vote` and `verified_purchase`. We decided to remove these because reviews without a helpful vote proably isn't very high quality, or irrelevant. Similarly, if the purchase can't be verified we can't be certain that the review comes from a real user. Reviews with less than 10 words were also deemed to be of lesser quality; The goal is to find descriptive reviews to facilitate language processing. \n",
    "\n",
    "Lastly, we removed products that had less than 15 total reviews. These would needlessly bloat our dataset, and make it more noisy. A large portion of the dataset contains products with minimal engagement, these would not facilitate community detection well. We summarized the compression in the table:\n",
    "\n",
    "| Step                                                  | Description                                                  | Count / Shape          |\n",
    "|-------------------------------------------------------|--------------------------------------------------------------|------------------------|\n",
    "| **Initial raw dataset**                               | Total number of reviews loaded                               | 17,328,314             |\n",
    "|                                                       | Initial dataframe shape                                      | (17,328,314, 10)       |\n",
    "|                                                       | Unique users (`user_id`)                                     | 6,503,429              |\n",
    "|                                                       | Unique `parent_asin` values                                  | 747,764                |\n",
    "| **After filtering reviews with `helpful_vote â‰¥ 1`**   | Removed unhelpful or unused reviews                          | (4,325,435, 10)        |\n",
    "| **After removing short reviews (< 10 words)**         | Kept only meaningful reviews                                 | (3,795,557, 10)        |\n",
    "| **After keeping only `verified_purchase == True`**    | Removed potentially fake/unreliable reviews                  | (2,428,871, 10)        |\n",
    "| **Total words in cleaned dataset**                    | Word count of all remaining reviews                          | 196,353,406            |\n",
    "| **After removing products with < 15 reviews**         | Ensured statistical validity of products                     | (1,341,856, 10)        |\n",
    "|                                                       | Unique ASINs after filtering                                 | 34,333                 |\n",
    "\n",
    "Thus we are left with 34,333 unique Movies/Shows (Rows). Next, we need to decide how many features to use. As can be seen from the feature tables above - many of the columns are redundant information we wont be needing. After the initial removal of rows, the metadata .csv is ASIN matched. \n",
    "\n",
    "Then, we removed non-essential or redundant columns such as `verified_purchase`, `subtitle`, `images_x`, `features`, `images_y`, `videos`, `store`, `details`, `bought_together`, and `author`. After this cleanup, we're left with the following columns in our final merged dataset: `rating`, `review_title`, `text`, `asin`, `parent_asin`, `user_id`, `timestamp`, `helpful_vote`, `main_category`, `movie_title`, `average_rating`, `rating_number`, `description`, `price`, and `categories`.\n",
    "\n",
    "We have also added sentiment scores for each review using Vader NLTK.\n",
    "\n",
    "The final dataset has the shape (1341856, 17) (with sentiment scores). Taking up ~0.75GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2937fcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download link: https://amazon-reviews-2023.github.io/\n",
    "\n",
    "# REVIEWS DATASET:::\n",
    "\n",
    "input_path = \"Movies_and_TV.jsonl.gz\"\n",
    "# Give the output CSV a name:\n",
    "output_path = \"Movies_and_TV_Full.csv\" #5.8GB file\n",
    "\n",
    "# Check if the output file already exists\n",
    "if os.path.exists(output_path):\n",
    "    print(f\"The file '{output_path}' already exists.\")\n",
    "else:\n",
    "    # Depends on available RAM, if set too high can cause crashing.\n",
    "    chunk_size = 150000\n",
    "    buffer = []\n",
    "    is_first_chunk = True\n",
    "\n",
    "    with gzip.open(input_path, 'rt', encoding='utf-8') as f:\n",
    "        total_lines = sum(1 for line in f)  # Count total lines for progress bar\n",
    "\n",
    "    with gzip.open(input_path, 'rt', encoding='utf-8') as f:\n",
    "        for i, line in tqdm(enumerate(f, 1), total=total_lines, desc=\"Loading reviews\", unit=\"line\"):\n",
    "            buffer.append(json.loads(line))\n",
    "\n",
    "            if i % chunk_size == 0:\n",
    "                df_chunk = pd.DataFrame(buffer)\n",
    "                df_chunk.to_csv(output_path, mode='w' if is_first_chunk else 'a',\n",
    "                                header=is_first_chunk, index=False)\n",
    "                buffer = []\n",
    "                is_first_chunk = False\n",
    "\n",
    "    # Final chunk\n",
    "    if buffer:\n",
    "        df_chunk = pd.DataFrame(buffer)\n",
    "        df_chunk.to_csv(output_path, mode='w' if is_first_chunk else 'a',\n",
    "                        header=is_first_chunk, index=False)\n",
    "        print(\"Reviews conversion finished\")\n",
    "\n",
    "# METADATA DATASET:::\n",
    "\n",
    "meta_input_path = \"meta_Movies_and_TV.jsonl.gz\"\n",
    "meta_output_path = \"meta_Movies_and_TV_Full.csv\"\n",
    "\n",
    "# Skip conversion if file already exists\n",
    "if os.path.exists(meta_output_path):\n",
    "    print(f\"The file '{meta_output_path}' already exists.\")\n",
    "else:\n",
    "    chunk_size = 150000\n",
    "    buffer = []\n",
    "    is_first_chunk = True\n",
    "\n",
    "    # Count total lines for progress bar\n",
    "    with gzip.open(meta_input_path, 'rt', encoding='utf-8') as f:\n",
    "        total_lines = sum(1 for _ in f)\n",
    "\n",
    "    with gzip.open(meta_input_path, 'rt', encoding='utf-8') as f:\n",
    "        for i, line in tqdm(enumerate(f, 1), total=total_lines, desc=\"Converting metadata\", unit=\"line\"):\n",
    "            buffer.append(json.loads(line))\n",
    "\n",
    "            if i % chunk_size == 0:\n",
    "                df_chunk = pd.DataFrame(buffer)\n",
    "                df_chunk.to_csv(meta_output_path, mode='w' if is_first_chunk else 'a',\n",
    "                                header=is_first_chunk, index=False)\n",
    "                buffer = []\n",
    "                is_first_chunk = False\n",
    "\n",
    "    # Final chunk\n",
    "    if buffer:\n",
    "        df_chunk = pd.DataFrame(buffer)\n",
    "        df_chunk.to_csv(meta_output_path, mode='w' if is_first_chunk else 'a',\n",
    "                        header=is_first_chunk, index=False)\n",
    "\n",
    "    print(\"Metadata conversion finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b962a705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After saving lets load the CSV and look at summary statistics:\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv(\"Movies_and_TV_Full.csv\")\n",
    "# Dataset dimensions\n",
    "print(f\"Total reviews: {len(df):,}\")\n",
    "print(f\"Dataframe shape before clean: {df.shape}\")\n",
    "print()\n",
    "\n",
    "# Check uniqueness\n",
    "print(f\"- Unique users (user_id): {df['user_id'].nunique():,}\")\n",
    "print(f\"- Unique products (asin): {df['asin'].nunique():,}\")\n",
    "\n",
    "if 'parent_asin' in df.columns:\n",
    "    print(f\"- Unique parent_asin: {df['parent_asin'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b653528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we clean the entire dataset before merging with metadata:\n",
    "\n",
    "review_length = 10\n",
    "review_threshold = 15 #greatly varies the size of the final network\n",
    "helpful_threshold = 1\n",
    "\n",
    "# Remove rows where 'helpful_vote' is 0\n",
    "df_cleaned = df[df['helpful_vote'] >= helpful_threshold]\n",
    "print(f\"Shape after removing unhelpful: {df_cleaned.shape}\")\n",
    "\n",
    "# Remove rows with less tan `review_length` words in 'text'\n",
    "df_cleaned = df_cleaned[df_cleaned['text'].apply(lambda x: len(str(x).split()) >= review_length)]\n",
    "print(f\"Shape after removing sub {review_length} word reviews: {df_cleaned.shape}\")\n",
    "\n",
    "# Remove rows where 'verified_purchase' is False\n",
    "df_cleaned = df_cleaned[df_cleaned['verified_purchase'] == True]\n",
    "print(f\"Shape after removing unverified purchases: {df_cleaned.shape}\")\n",
    "\n",
    "# Calculate the total number of words in the 'text' column after cleaning\n",
    "total_words = df_cleaned['text'].apply(lambda x: len(str(x).split())).sum()\n",
    "print(f\"Total number of words in the dataset after cleaning: {total_words:,}\")\n",
    "\n",
    "# Count reviews per ASIN\n",
    "asin_review_counts = df_cleaned['asin'].value_counts()\n",
    "\n",
    "# Filter ASINs based on the review threshold\n",
    "valid_asins = asin_review_counts[asin_review_counts >= review_threshold].index\n",
    "\n",
    "# Keep only rows where the ASIN is in the list of valid ASINs\n",
    "df_cleaned = df_cleaned[df_cleaned['asin'].isin(valid_asins)]\n",
    "print(f\"Shape after filtering products with less than {review_threshold} reviews: {df_cleaned.shape}\")\n",
    "\n",
    "print(f\"Unique ASINs after filtering: {df_cleaned['asin'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d079ef80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets are merged:\n",
    "\n",
    "df_meta = pd.read_csv(\"meta_Movies_and_TV_Full.csv\")\n",
    "df_merged = pd.merge(df_cleaned, df_meta, left_on='parent_asin', right_on='parent_asin', how='left')\n",
    "print(\"Merged shape (before dropping):\", df_merged.shape)\n",
    "#Drop columns we don't need at the moment\n",
    "columns_to_drop = [\n",
    "    'verified_purchase', 'subtitle', 'images_x', 'features', 'images_y',\n",
    "    'videos', 'store', 'details', 'bought_together', 'author'\n",
    "]\n",
    "\n",
    "df_merged.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
    "\n",
    "df_merged.rename(columns={\n",
    "    'title_x': 'review_title',    \n",
    "    'title_y': 'movie_title'\n",
    "}, inplace=True)\n",
    "\n",
    "print(\"Merge Complete:\")\n",
    "print(\"Merged shape:\", df_merged.shape)\n",
    "df_merged.to_csv(\"Merged_Reviews_and_Metadata.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680bc176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we want to add sentiment data with NLTK:\n",
    "\n",
    "threads = 8\n",
    "neutral_threshold = 0.1  # Bin around 0 that indicate a neutral sentiment\n",
    "\n",
    "# Function to process a batch of reviews\n",
    "def analyze_batch_sentiment(batch):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    return [sia.polarity_scores(str(text))['compound'] for text in batch]\n",
    "\n",
    "# More is faster but uses more resources\n",
    "batch_size = 10000\n",
    "\n",
    "# Batch Generator\n",
    "batches = [df_merged['text'][i:i + batch_size] for i in range(0, len(df_merged), batch_size)]\n",
    "\n",
    "with tqdm_joblib(desc=\"Sentiment Analysis\", total=len(batches)):\n",
    "    sentiment_scores_batches = Parallel(n_jobs=threads)(\n",
    "        delayed(analyze_batch_sentiment)(batch) for batch in batches\n",
    "    )\n",
    "\n",
    "# Flatten the list of batches into a single list\n",
    "sentiment_scores = [score for batch_scores in sentiment_scores_batches for score in batch_scores]\n",
    "\n",
    "# Add sentiment score and category to dataframe\n",
    "df_merged['sentiment_score'] = sentiment_scores\n",
    "df_merged['sentiment_category'] = df_merged['sentiment_score'].apply(\n",
    "    lambda x: 'positive' if x > neutral_threshold else ('negative' if x < -neutral_threshold else 'neutral')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767cba32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned dataset with sentiment:\n",
    "output_path = \"Movies_and_TV_Cleaned_Sentiment.csv\"\n",
    "df_merged.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ba5d45",
   "metadata": {},
   "source": [
    "Next up is our graph. The choice of graph is what we call a \"Co-Reviews\" graph. The co reviews refers to the fact that an edge is formed when two products have been reviewed by the same person. So, if movie X and movie Y has been reviewed by some user, they receive an edge weight of +1 between them. In this way, we create a graph where each node is a Movie/Show (essentially an ASIN product code), each product has at least 16 reviews attatched to them and this is where the text analysis comes into play. \n",
    "\n",
    "Therefore, our graph connects movies together that communities enjoy watching - which is one of our goals of this project. The immediate issue with this approach is that some people are very prolific where the vast majority of people write few reviews. Therefore a small amount of single individuals can baloon the edge count making the graph noisy. We therefore prune all edges with weight less than 2, to make sure two or more people have reviewed each pair of ASIN's in order for the node to survive in our final graph.\n",
    "\n",
    "This leaves us with; Node count: 20711 and edge count: 91728.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f77cee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use an optimized algorithm to compute the graph in batches\n",
    "# Even with this it takes 30+ minutes on a 6 core processor. \n",
    "\n",
    "# 1. Load and preprocess data efficiently\n",
    "print(\"Loading data...\")\n",
    "df_graph = df_merged[['user_id', 'parent_asin']].dropna().drop_duplicates()\n",
    "\n",
    "# 2. Create bidirectional mappings\n",
    "print(\"Creating mappings...\")\n",
    "# ASIN to reviewers\n",
    "asin_to_reviewers = defaultdict(set)\n",
    "# Reviewer to ASINs\n",
    "reviewer_to_asins = defaultdict(set)\n",
    "\n",
    "for _, row in tqdm(df_graph.iterrows(), total=len(df_graph)):\n",
    "    asin_to_reviewers[row['parent_asin']].add(row['user_id'])\n",
    "    reviewer_to_asins[row['user_id']].add(row['parent_asin'])\n",
    "\n",
    "# 3. Create graph and add nodes\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(asin_to_reviewers.keys())\n",
    "\n",
    "# 4. Parallel edge computation\n",
    "def compute_edges_for_asin(asin, asin_to_reviewers, all_asins):\n",
    "    edges = []\n",
    "    # Get all other ASINs that share at least one reviewer\n",
    "    co_reviewed_asins = set()\n",
    "    for reviewer in asin_to_reviewers[asin]:\n",
    "        co_reviewed_asins.update(reviewer_to_asins[reviewer])\n",
    "    \n",
    "    # Remove self and already processed pairs\n",
    "    co_reviewed_asins.discard(asin)\n",
    "    co_reviewed_asins = [a for a in co_reviewed_asins if all_asins.index(a) > all_asins.index(asin)]\n",
    "    \n",
    "    for other_asin in co_reviewed_asins:\n",
    "        weight = len(asin_to_reviewers[asin] & asin_to_reviewers[other_asin])\n",
    "        if weight > 0:\n",
    "            edges.append((asin, other_asin, weight))\n",
    "    return edges\n",
    "\n",
    "print(\"Computing edges in parallel...\")\n",
    "all_asins = list(asin_to_reviewers.keys())\n",
    "results = Parallel(n_jobs=-1, prefer=\"threads\")(\n",
    "    delayed(compute_edges_for_asin)(asin, asin_to_reviewers, all_asins)\n",
    "    for asin in tqdm(all_asins)\n",
    ")\n",
    "\n",
    "# 5. Add edges to graph\n",
    "print(\"Building graph...\")\n",
    "for edges in tqdm(results):\n",
    "    for asin1, asin2, weight in edges:\n",
    "        G.add_edge(asin1, asin2, weight=weight)\n",
    "\n",
    "print(f\"Graph constructed with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de296449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the graph as a json file\n",
    "graph_data = json_graph.node_link_data(G)\n",
    "with open(\"network.json\", \"w\") as f:\n",
    "    json.dump(graph_data, f, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29090ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to open up the graph if needed:\n",
    "with open(\"network.json\", \"r\") as f:\n",
    "    graph_data = json.load(f)\n",
    "\n",
    "G = json_graph.node_link_graph(graph_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd835b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets clean up the graph:\n",
    "MIN_SHARED_REVIEWERS = 2 \n",
    "\n",
    "G_filtered = nx.Graph()\n",
    "for u, v, data in G.edges(data=True):\n",
    "    if data['weight'] >= MIN_SHARED_REVIEWERS:\n",
    "        G_filtered.add_edge(u, v, weight=data['weight'])\n",
    "\n",
    "print(f\"Filtered Graph: {G_filtered.number_of_nodes()} nodes, {G_filtered.number_of_edges()} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2a5810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final graph with pruned edges:\n",
    "graph_data = json_graph.node_link_data(G_filtered)\n",
    "with open(\"network_filtered.json\", \"w\") as f:\n",
    "    json.dump(graph_data, f, indent = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea257a33",
   "metadata": {},
   "source": [
    "# Part III"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b07bca",
   "metadata": {},
   "source": [
    "The following is the graph statistics used in the project. Here we show how all the plots work, and how we calculated the coefficients and network measures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee680af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataframe for the plots:\n",
    "df = pd.read_csv(\"Movies_and_TV_Cleaned_Sentiment.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6fd796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we plot the user and product distributions: \n",
    "\n",
    "# Compute reviews per product\n",
    "asin_review_counts = df['asin'].value_counts()\n",
    "\n",
    "# Compute reviews per user\n",
    "user_review_counts = df['user_id'].value_counts()\n",
    "filtered_user_counts = user_review_counts[user_review_counts <= 10000]\n",
    "\n",
    "# Plot styling\n",
    "sns.set(style=\"whitegrid\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6), sharey=True)\n",
    "\n",
    "# --- Plot 1: Reviews per Product ---\n",
    "sns.histplot(\n",
    "    asin_review_counts,\n",
    "    bins=50,\n",
    "    binrange=(0, asin_review_counts.max()),\n",
    "    color=\"#4C72B0\",\n",
    "    edgecolor='white',\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_yscale(\"log\")\n",
    "axes[0].set_title(\"Reviews per Product\", fontsize=14)\n",
    "axes[0].set_xlabel(\"Number of Reviews\", fontsize=12)\n",
    "axes[0].set_ylabel(\"Number of Products (log scale)\", fontsize=12)\n",
    "axes[0].grid(True, linestyle='--', linewidth=0.5)\n",
    "\n",
    "# --- Plot 2: Reviews per User (Filtered) ---\n",
    "sns.histplot(\n",
    "    filtered_user_counts,\n",
    "    bins=50,\n",
    "    color=\"#4C72B0\",\n",
    "    edgecolor='white',\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_yscale(\"log\")\n",
    "axes[1].set_title(\"Reviews per User\", fontsize=14)\n",
    "axes[1].set_xlabel(\"Number of Reviews by User\", fontsize=12)\n",
    "axes[1].set_ylabel(\"\")  # Shared y-axis already labeled\n",
    "axes[1].grid(True, linestyle='--', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89316bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot made to resemble the style in the course notes :::\n",
    "# This plot shows us the degree distribution with a comparison to a random graph\n",
    "\n",
    "G_real = G_filtered\n",
    "\n",
    "# --- Step 2: Generate a random graph for comparison\n",
    "n = G_real.number_of_nodes()\n",
    "p = (2 * G_real.number_of_edges()) / (n * (n - 1))  # match average degree\n",
    "G_random = nx.erdos_renyi_graph(n, p)\n",
    "\n",
    "# --- Step 3: Get degree distributions\n",
    "def degree_distribution(graph):\n",
    "    degrees = [d for n, d in graph.degree()]\n",
    "    hist = np.bincount(degrees)\n",
    "    pk = hist / sum(hist)\n",
    "    return np.nonzero(pk)[0], pk[pk > 0]  # degrees, probabilities\n",
    "\n",
    "deg_real, pk_real = degree_distribution(G_real)\n",
    "deg_rand, pk_rand = degree_distribution(G_random)\n",
    "\n",
    "# --- Step 4: Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.loglog(deg_rand, pk_rand, 'o-', color='red', label=\"Random Network\")\n",
    "plt.loglog(deg_real, pk_real, 'o-', color='blue', label=\"Amazon Network\")\n",
    "\n",
    "# --- Step 5: Annotate with average/median degree\n",
    "avg_deg_real = np.mean([d for _, d in G_real.degree()])\n",
    "med_deg_real = np.median([d for _, d in G_real.degree()])\n",
    "avg_deg_rand = np.mean([d for _, d in G_random.degree()])\n",
    "\n",
    "# Add vertical lines\n",
    "plt.axvline(avg_deg_rand, linestyle=\"--\", color=\"yellow\", label=f\"Avg Degree (Random): {avg_deg_rand:.2f}\")\n",
    "plt.axvline(avg_deg_real, linestyle=\"--\", color=\"teal\", label=f\"Avg Degree (Amazon Network): {avg_deg_real:.2f}\")\n",
    "plt.axvline(med_deg_real, linestyle=\"--\", color=\"black\", label=f\"Median Degree (Amazon Network): {med_deg_real:.0f}\")\n",
    "\n",
    "# Labels and legend\n",
    "plt.xlabel(\"Degree k (node)\")\n",
    "plt.ylabel(\"Probability p(k) of degree k\")\n",
    "plt.title(\"Degree Distribution of Amazon Reviews Network vs. Random Network\")\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"both\", linestyle='--', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad88c1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code morphed from previous assignments:\n",
    "# Calculating the clustering coefficients\n",
    "# This shows clear evidence of the small world property!\n",
    "\n",
    "# Step 1: Extract giant component of real network\n",
    "largest_cc = max(nx.connected_components(G_filtered), key=len)\n",
    "G_giant = G_filtered.subgraph(largest_cc).copy()\n",
    "\n",
    "# Step 2: Generate a random network with same size and edge count\n",
    "n = G_giant.number_of_nodes()\n",
    "m = G_giant.number_of_edges()\n",
    "p = (2 * m) / (n * (n - 1))\n",
    "G_random = nx.erdos_renyi_graph(n, p)\n",
    "\n",
    "# Step 3: Approximate average path length (sampling)\n",
    "def approximate_avg_path_length(graph, samples=10000):\n",
    "    nodes = list(graph.nodes())\n",
    "    total_length = 0\n",
    "    valid_samples = 0\n",
    "    for _ in range(samples):\n",
    "        u, v = np.random.choice(nodes, 2, replace=False)\n",
    "        try:\n",
    "            length = nx.shortest_path_length(graph, source=u, target=v)\n",
    "            total_length += length\n",
    "            valid_samples += 1\n",
    "        except nx.NetworkXNoPath:\n",
    "            continue\n",
    "    return total_length / valid_samples if valid_samples > 0 else float('inf')\n",
    "\n",
    "# Step 4: Compute metrics\n",
    "real_path = approximate_avg_path_length(G_giant)\n",
    "rand_path = approximate_avg_path_length(G_random)\n",
    "real_clustering = nx.average_clustering(G_giant)\n",
    "rand_clustering = nx.average_clustering(G_random)\n",
    "\n",
    "# Step 5: Print comparison\n",
    "print(\"=== Small-World Property Comparison ===\")\n",
    "print(f\"Approx. Avg Path Length (Amazon Network): {real_path:.4f}\")\n",
    "print(f\"Approx. Avg Path Length (Random Network): {rand_path:.4f}\")\n",
    "print()\n",
    "print(f\"Clustering Coefficient (Amazon Network):  {real_clustering:.4f}\")\n",
    "print(f\"Clustering Coefficient (Random Network):  {rand_clustering:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740f4624",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = G_filtered.number_of_nodes()\n",
    "m = G_filtered.number_of_edges()\n",
    "\n",
    "p_actual = (2 * m) / (n * (n - 1))\n",
    "p_critical = 1 / n\n",
    "p_connected = np.log(n) / n\n",
    "\n",
    "print(\"=== Connectivity Analysis ===\")\n",
    "print(f\"Number of nodes (n):                    {n}\")\n",
    "print(f\"Number of edges (m):                    {m}\")\n",
    "print(f\"Actual edge probability (p):            {p_actual:.6f}\")\n",
    "print(f\"Critical threshold (1/n):               {p_critical:.6f}\")\n",
    "print(f\"Connected threshold (log(n)/n):         {p_connected:.6f}\")\n",
    "print()\n",
    "print(f\"Giant Component Exists?   {p_actual > p_critical}\")\n",
    "print(f\"Graph Likely Fully Connected?   {p_actual > p_connected}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299797b0",
   "metadata": {},
   "source": [
    "To summarize, it's clear that the network falls into the \"supercritical\" regime. There is clear evidence of the small world property, as is expected with natural networks. Additionally, we demonstrate how it differs from the random network that lacks the same clustering coefficient. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8f8cc9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
