{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a6705b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Friis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import gzip\n",
    "import json\n",
    "import ast\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Data manipulation and numerical operations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Parallel processing and progress tracking\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from tqdm_joblib import tqdm_joblib\n",
    "\n",
    "# Natural Language Processing\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Graph and network analysis\n",
    "import networkx as nx\n",
    "from networkx.readwrite import json_graph\n",
    "import netwulf as nw\n",
    "import community as community_louvain  # Louvain algorithm for community detection\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Use with the image plotter\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image \n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Set random seed for reproducibility (Louvain algorithm)\n",
    "np.random.seed(seed=100)  # Does it work???\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc14332",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "Motivation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5964b389",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e353696",
   "metadata": {},
   "source": [
    "Our dataset is a subset of the \"Amazon Reviews\" dataset collected in 2023 by McAuley Lab:\n",
    "\n",
    "Link to dataset: https://amazon-reviews-2023.github.io/\n",
    "\n",
    "We are not interested in the entire 571M+ reviews but will be looking specifically at the \"Movies_and_TV\" subset. Each subset is divided into two .csv files, a review and a metadata.\n",
    "\n",
    "The review contains - as the name would suggest - the reviews themselves. With zero cleaning, the combined dataset is around 7GB. Almost immediately there are large portions of the dataset we deem to not be relevant data and so we will be shrinking the usable dataset, to get a much cleaner and more potent dataset for the purposes of creating the co-reviewer graph. \n",
    "\n",
    "Firstly, an overview of the columns in each dataset:\n",
    "\n",
    "| Reviews             | Description                                                                 |\n",
    "|--------------------|-----------------------------------------------------------------------------|\n",
    "| `rating`           | Star rating given by the reviewer (e.g., 1 to 5)                            |\n",
    "| `title`            | Title or headline of the review                                             |\n",
    "| `text`             | The main body text of the review                                            |\n",
    "| `images`           | Image URLs or attachments included with the review (if any)                 |\n",
    "| `asin`             | Amazon Standard Identification Number for the specific product              |\n",
    "| `parent_asin`      | Group identifier to cluster product variants                                |\n",
    "| `user_id`          | Anonymized unique identifier for the reviewer                               |\n",
    "| `timestamp`        | Time the review was posted (UNIX or ISO format)                             |\n",
    "| `helpful_vote`     | Number of people who found the review helpful                               |\n",
    "| `verified_purchase`| Boolean indicating whether the purchase was verified by Amazon             |\n",
    "\n",
    "| Metadata           | Description                                                                 |\n",
    "|------------------|-----------------------------------------------------------------------------|\n",
    "| `main_category`  | The top-level category the product belongs to (e.g., \"Movies & TV\")         |\n",
    "| `title`          | The main product title (often the movie/show title)                         |\n",
    "| `subtitle`       | Optional subtitle for the product (e.g., edition/version info)              |\n",
    "| `average_rating` | The average customer rating for the product                                 |\n",
    "| `rating_number`  | Total number of ratings the product has received                            |\n",
    "| `features`       | A list of product features (e.g., language, format)                         |\n",
    "| `description`    | A longer description or summary of the product                              |\n",
    "| `price`          | The listed price of the product                                              |\n",
    "| `images`         | A list of image URLs associated with the product                            |\n",
    "| `videos`         | Video media links (e.g., trailers or previews), if available                |\n",
    "| `store`          | The Amazon store/subcategory under which the product is listed              |\n",
    "| `categories`     | List of categories/tags assigned to the product (e.g., genre, theme)        |\n",
    "| `details`        | Additional technical or marketing metadata (format, region, etc.)           |\n",
    "| `parent_asin`    | A group-level identifier for versions of the same product                   |\n",
    "\n",
    "\n",
    "A couple of these drastically reduce the size like, `helpful_vote` and `verified_purchase`. We decided to remove these because reviews without a helpful vote proably isn't very high quality, or irrelevant. Similarly, if the purchase can't be verified we can't be certain that the review comes from a real user. Reviews with less than 10 words were also deemed to be of lesser quality; The goal is to find descriptive reviews to facilitate language processing. \n",
    "\n",
    "Lastly, we removed products that had less than 15 total reviews. These would needlessly bloat our dataset, and make it more noisy. A large portion of the dataset contains products with minimal engagement, these would not facilitate community detection well. We summarized the compression in the table:\n",
    "\n",
    "| Step                                                  | Description                                                  | Count / Shape          |\n",
    "|-------------------------------------------------------|--------------------------------------------------------------|------------------------|\n",
    "| **Initial raw dataset**                               | Total number of reviews loaded                               | 17,328,314             |\n",
    "|                                                       | Initial dataframe shape                                      | (17,328,314, 10)       |\n",
    "|                                                       | Unique users (`user_id`)                                     | 6,503,429              |\n",
    "|                                                       | Unique `parent_asin` values                                  | 747,764                |\n",
    "| **After filtering reviews with `helpful_vote â‰¥ 1`**   | Removed unhelpful or unused reviews                          | (4,325,435, 10)        |\n",
    "| **After removing short reviews (< 10 words)**         | Kept only meaningful reviews                                 | (3,795,557, 10)        |\n",
    "| **After keeping only `verified_purchase == True`**    | Removed potentially fake/unreliable reviews                  | (2,428,871, 10)        |\n",
    "| **Total words in cleaned dataset**                    | Word count of all remaining reviews                          | 196,353,406            |\n",
    "| **After removing products with < 15 reviews**         | Ensured statistical validity of products                     | (1,341,856, 10)        |\n",
    "|                                                       | Unique ASINs after filtering                                 | 34,333                 |\n",
    "\n",
    "Thus we are left with 34,333 unique Movies/Shows (Rows). Next, we need to decide how many features to use. As can be seen from the feature tables above - many of the columns are redundant information we wont be needing. After the initial removal of rows, the metadata .csv is ASIN matched. \n",
    "\n",
    "Then, we removed non-essential or redundant columns such as `verified_purchase`, `subtitle`, `images_x`, `features`, `images_y`, `videos`, `store`, `details`, `bought_together`, and `author`. After this cleanup, we're left with the following columns in our final merged dataset: `rating`, `review_title`, `text`, `asin`, `parent_asin`, `user_id`, `timestamp`, `helpful_vote`, `main_category`, `movie_title`, `average_rating`, `rating_number`, `description`, `price`, and `categories`.\n",
    "\n",
    "We have also added sentiment scores for each review using Vader NLTK.\n",
    "\n",
    "The final dataset has the shape (1341856, 17) (with sentiment scores). Taking up ~0.75GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2937fcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download link: https://amazon-reviews-2023.github.io/\n",
    "\n",
    "# REVIEWS DATASET:::\n",
    "\n",
    "input_path = \"Movies_and_TV.jsonl.gz\"\n",
    "# Give the output CSV a name:\n",
    "output_path = \"Movies_and_TV_Full.csv\" #5.8GB file\n",
    "\n",
    "# Check if the output file already exists\n",
    "if os.path.exists(output_path):\n",
    "    print(f\"The file '{output_path}' already exists.\")\n",
    "else:\n",
    "    # Depends on available RAM, if set too high can cause crashing.\n",
    "    chunk_size = 150000\n",
    "    buffer = []\n",
    "    is_first_chunk = True\n",
    "\n",
    "    with gzip.open(input_path, 'rt', encoding='utf-8') as f:\n",
    "        total_lines = sum(1 for line in f)  # Count total lines for progress bar\n",
    "\n",
    "    with gzip.open(input_path, 'rt', encoding='utf-8') as f:\n",
    "        for i, line in tqdm(enumerate(f, 1), total=total_lines, desc=\"Loading reviews\", unit=\"line\"):\n",
    "            buffer.append(json.loads(line))\n",
    "\n",
    "            if i % chunk_size == 0:\n",
    "                df_chunk = pd.DataFrame(buffer)\n",
    "                df_chunk.to_csv(output_path, mode='w' if is_first_chunk else 'a',\n",
    "                                header=is_first_chunk, index=False)\n",
    "                buffer = []\n",
    "                is_first_chunk = False\n",
    "\n",
    "    # Final chunk\n",
    "    if buffer:\n",
    "        df_chunk = pd.DataFrame(buffer)\n",
    "        df_chunk.to_csv(output_path, mode='w' if is_first_chunk else 'a',\n",
    "                        header=is_first_chunk, index=False)\n",
    "        print(\"Reviews conversion finished\")\n",
    "\n",
    "# METADATA DATASET:::\n",
    "\n",
    "meta_input_path = \"meta_Movies_and_TV.jsonl.gz\"\n",
    "meta_output_path = \"meta_Movies_and_TV_Full.csv\"\n",
    "\n",
    "# Skip conversion if file already exists\n",
    "if os.path.exists(meta_output_path):\n",
    "    print(f\"The file '{meta_output_path}' already exists.\")\n",
    "else:\n",
    "    chunk_size = 150000\n",
    "    buffer = []\n",
    "    is_first_chunk = True\n",
    "\n",
    "    # Count total lines for progress bar\n",
    "    with gzip.open(meta_input_path, 'rt', encoding='utf-8') as f:\n",
    "        total_lines = sum(1 for _ in f)\n",
    "\n",
    "    with gzip.open(meta_input_path, 'rt', encoding='utf-8') as f:\n",
    "        for i, line in tqdm(enumerate(f, 1), total=total_lines, desc=\"Converting metadata\", unit=\"line\"):\n",
    "            buffer.append(json.loads(line))\n",
    "\n",
    "            if i % chunk_size == 0:\n",
    "                df_chunk = pd.DataFrame(buffer)\n",
    "                df_chunk.to_csv(meta_output_path, mode='w' if is_first_chunk else 'a',\n",
    "                                header=is_first_chunk, index=False)\n",
    "                buffer = []\n",
    "                is_first_chunk = False\n",
    "\n",
    "    # Final chunk\n",
    "    if buffer:\n",
    "        df_chunk = pd.DataFrame(buffer)\n",
    "        df_chunk.to_csv(meta_output_path, mode='w' if is_first_chunk else 'a',\n",
    "                        header=is_first_chunk, index=False)\n",
    "\n",
    "    print(\"Metadata conversion finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b962a705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After saving lets load the CSV and look at summary statistics:\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv(\"Movies_and_TV_Full.csv\")\n",
    "# Dataset dimensions\n",
    "print(f\"Total reviews: {len(df):,}\")\n",
    "print(f\"Dataframe shape before clean: {df.shape}\")\n",
    "print()\n",
    "\n",
    "# Check uniqueness\n",
    "print(f\"- Unique users (user_id): {df['user_id'].nunique():,}\")\n",
    "print(f\"- Unique products (asin): {df['asin'].nunique():,}\")\n",
    "\n",
    "if 'parent_asin' in df.columns:\n",
    "    print(f\"- Unique parent_asin: {df['parent_asin'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b653528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we clean the entire dataset before merging with metadata:\n",
    "\n",
    "review_length = 10\n",
    "review_threshold = 15 #greatly varies the size of the final network\n",
    "helpful_threshold = 1\n",
    "\n",
    "# Remove rows where 'helpful_vote' is 0\n",
    "df_cleaned = df[df['helpful_vote'] >= helpful_threshold]\n",
    "print(f\"Shape after removing unhelpful: {df_cleaned.shape}\")\n",
    "\n",
    "# Remove rows with less tan `review_length` words in 'text'\n",
    "df_cleaned = df_cleaned[df_cleaned['text'].apply(lambda x: len(str(x).split()) >= review_length)]\n",
    "print(f\"Shape after removing sub {review_length} word reviews: {df_cleaned.shape}\")\n",
    "\n",
    "# Remove rows where 'verified_purchase' is False\n",
    "df_cleaned = df_cleaned[df_cleaned['verified_purchase'] == True]\n",
    "print(f\"Shape after removing unverified purchases: {df_cleaned.shape}\")\n",
    "\n",
    "# Calculate the total number of words in the 'text' column after cleaning\n",
    "total_words = df_cleaned['text'].apply(lambda x: len(str(x).split())).sum()\n",
    "print(f\"Total number of words in the dataset after cleaning: {total_words:,}\")\n",
    "\n",
    "# Count reviews per ASIN\n",
    "asin_review_counts = df_cleaned['asin'].value_counts()\n",
    "\n",
    "# Filter ASINs based on the review threshold\n",
    "valid_asins = asin_review_counts[asin_review_counts >= review_threshold].index\n",
    "\n",
    "# Keep only rows where the ASIN is in the list of valid ASINs\n",
    "df_cleaned = df_cleaned[df_cleaned['asin'].isin(valid_asins)]\n",
    "print(f\"Shape after filtering products with less than {review_threshold} reviews: {df_cleaned.shape}\")\n",
    "\n",
    "print(f\"Unique ASINs after filtering: {df_cleaned['asin'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d079ef80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets are merged:\n",
    "\n",
    "df_meta = pd.read_csv(\"meta_Movies_and_TV_Full.csv\")\n",
    "df_merged = pd.merge(df_cleaned, df_meta, left_on='parent_asin', right_on='parent_asin', how='left')\n",
    "print(\"Merged shape (before dropping):\", df_merged.shape)\n",
    "#Drop columns we don't need at the moment\n",
    "columns_to_drop = [\n",
    "    'verified_purchase', 'subtitle', 'images_x', 'features', 'images_y',\n",
    "    'videos', 'store', 'details', 'bought_together', 'author'\n",
    "]\n",
    "\n",
    "df_merged.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
    "\n",
    "df_merged.rename(columns={\n",
    "    'title_x': 'review_title',    \n",
    "    'title_y': 'movie_title'\n",
    "}, inplace=True)\n",
    "\n",
    "print(\"Merge Complete:\")\n",
    "print(\"Merged shape:\", df_merged.shape)\n",
    "df_merged.to_csv(\"Merged_Reviews_and_Metadata.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680bc176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we want to add sentiment data with NLTK:\n",
    "\n",
    "threads = 8\n",
    "neutral_threshold = 0.1  # Bin around 0 that indicate a neutral sentiment\n",
    "\n",
    "# Function to process a batch of reviews\n",
    "def analyze_batch_sentiment(batch):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    return [sia.polarity_scores(str(text))['compound'] for text in batch]\n",
    "\n",
    "# More is faster but uses more resources\n",
    "batch_size = 10000\n",
    "\n",
    "# Batch Generator\n",
    "batches = [df_merged['text'][i:i + batch_size] for i in range(0, len(df_merged), batch_size)]\n",
    "\n",
    "with tqdm_joblib(desc=\"Sentiment Analysis\", total=len(batches)):\n",
    "    sentiment_scores_batches = Parallel(n_jobs=threads)(\n",
    "        delayed(analyze_batch_sentiment)(batch) for batch in batches\n",
    "    )\n",
    "\n",
    "# Flatten the list of batches into a single list\n",
    "sentiment_scores = [score for batch_scores in sentiment_scores_batches for score in batch_scores]\n",
    "\n",
    "# Add sentiment score and category to dataframe\n",
    "df_merged['sentiment_score'] = sentiment_scores\n",
    "df_merged['sentiment_category'] = df_merged['sentiment_score'].apply(\n",
    "    lambda x: 'positive' if x > neutral_threshold else ('negative' if x < -neutral_threshold else 'neutral')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767cba32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned dataset with sentiment:\n",
    "output_path = \"Movies_and_TV_Cleaned_Sentiment.csv\"\n",
    "df_merged.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ba5d45",
   "metadata": {},
   "source": [
    "Next up is our graph. The choice of graph is what we call a \"Co-Reviews\" graph. The co reviews refers to the fact that an edge is formed when two products have been reviewed by the same person. So, if movie X and movie Y has been reviewed by some user, they receive an edge weight of +1 between them. In this way, we create a graph where each node is a Movie/Show (essentially an ASIN product code), each product has at least 16 reviews attatched to them and this is where the text analysis comes into play. \n",
    "\n",
    "Therefore, our graph connects movies together that communities enjoy watching - which is one of our goals of this project. The immediate issue with this approach is that some people are very prolific where the vast majority of people write few reviews. Therefore a small amount of single individuals can baloon the edge count making the graph noisy. We therefore prune all edges with weight less than 2, to make sure two or more people have reviewed each pair of ASIN's in order for the node to survive in our final graph.\n",
    "\n",
    "This leaves us with; Node count: 20711 and edge count: 91728.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f77cee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use an optimized algorithm to compute the graph in batches\n",
    "# Even with this it takes 30+ minutes on a 6 core processor. \n",
    "\n",
    "# 1. Load and preprocess data efficiently\n",
    "print(\"Loading data...\")\n",
    "df_graph = df_merged[['user_id', 'parent_asin']].dropna().drop_duplicates()\n",
    "\n",
    "# 2. Create bidirectional mappings\n",
    "print(\"Creating mappings...\")\n",
    "# ASIN to reviewers\n",
    "asin_to_reviewers = defaultdict(set)\n",
    "# Reviewer to ASINs\n",
    "reviewer_to_asins = defaultdict(set)\n",
    "\n",
    "for _, row in tqdm(df_graph.iterrows(), total=len(df_graph)):\n",
    "    asin_to_reviewers[row['parent_asin']].add(row['user_id'])\n",
    "    reviewer_to_asins[row['user_id']].add(row['parent_asin'])\n",
    "\n",
    "# 3. Create graph and add nodes\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(asin_to_reviewers.keys())\n",
    "\n",
    "# 4. Parallel edge computation\n",
    "def compute_edges_for_asin(asin, asin_to_reviewers, all_asins):\n",
    "    edges = []\n",
    "    # Get all other ASINs that share at least one reviewer\n",
    "    co_reviewed_asins = set()\n",
    "    for reviewer in asin_to_reviewers[asin]:\n",
    "        co_reviewed_asins.update(reviewer_to_asins[reviewer])\n",
    "    \n",
    "    # Remove self and already processed pairs\n",
    "    co_reviewed_asins.discard(asin)\n",
    "    co_reviewed_asins = [a for a in co_reviewed_asins if all_asins.index(a) > all_asins.index(asin)]\n",
    "    \n",
    "    for other_asin in co_reviewed_asins:\n",
    "        weight = len(asin_to_reviewers[asin] & asin_to_reviewers[other_asin])\n",
    "        if weight > 0:\n",
    "            edges.append((asin, other_asin, weight))\n",
    "    return edges\n",
    "\n",
    "print(\"Computing edges in parallel...\")\n",
    "all_asins = list(asin_to_reviewers.keys())\n",
    "results = Parallel(n_jobs=-1, prefer=\"threads\")(\n",
    "    delayed(compute_edges_for_asin)(asin, asin_to_reviewers, all_asins)\n",
    "    for asin in tqdm(all_asins)\n",
    ")\n",
    "\n",
    "# 5. Add edges to graph\n",
    "print(\"Building graph...\")\n",
    "for edges in tqdm(results):\n",
    "    for asin1, asin2, weight in edges:\n",
    "        G.add_edge(asin1, asin2, weight=weight)\n",
    "\n",
    "print(f\"Graph constructed with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de296449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the graph as a json file\n",
    "# This is not strictly necessary to run since we save the pruned graph just below :)\n",
    "graph_data = json_graph.node_link_data(G)\n",
    "with open(\"network.json\", \"w\") as f:\n",
    "    json.dump(graph_data, f, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29090ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to open up the graph if needed:\n",
    "with open(\"network.json\", \"r\") as f:\n",
    "    graph_data = json.load(f)\n",
    "\n",
    "G = json_graph.node_link_graph(graph_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd835b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets clean up the graph:\n",
    "MIN_SHARED_REVIEWERS = 2 \n",
    "\n",
    "G_filtered = nx.Graph()\n",
    "for u, v, data in G.edges(data=True):\n",
    "    if data['weight'] >= MIN_SHARED_REVIEWERS:\n",
    "        G_filtered.add_edge(u, v, weight=data['weight'])\n",
    "\n",
    "print(f\"Filtered Graph: {G_filtered.number_of_nodes()} nodes, {G_filtered.number_of_edges()} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2a5810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final graph with pruned edges:\n",
    "graph_data = json_graph.node_link_data(G_filtered)\n",
    "with open(\"network_filtered.json\", \"w\") as f:\n",
    "    json.dump(graph_data, f, indent = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea257a33",
   "metadata": {},
   "source": [
    "# Part III"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b07bca",
   "metadata": {},
   "source": [
    "The following is the graph statistics used in the project. Here we show how all the plots work, and how we calculated the coefficients and network measures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee680af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataframe for the plots:\n",
    "df = pd.read_csv(\"Movies_and_TV_Cleaned_Sentiment.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6fd796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we plot the user and product distributions: \n",
    "\n",
    "# Compute reviews per product\n",
    "asin_review_counts = df['asin'].value_counts()\n",
    "\n",
    "# Compute reviews per user\n",
    "user_review_counts = df['user_id'].value_counts()\n",
    "filtered_user_counts = user_review_counts[user_review_counts <= 10000]\n",
    "\n",
    "# Plot styling\n",
    "sns.set(style=\"whitegrid\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6), sharey=True)\n",
    "\n",
    "# --- Plot 1: Reviews per Product ---\n",
    "sns.histplot(\n",
    "    asin_review_counts,\n",
    "    bins=50,\n",
    "    binrange=(0, asin_review_counts.max()),\n",
    "    color=\"#4C72B0\",\n",
    "    edgecolor='white',\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_yscale(\"log\")\n",
    "axes[0].set_title(\"Reviews per Product\", fontsize=14)\n",
    "axes[0].set_xlabel(\"Number of Reviews\", fontsize=12)\n",
    "axes[0].set_ylabel(\"Number of Products (log scale)\", fontsize=12)\n",
    "axes[0].grid(True, linestyle='--', linewidth=0.5)\n",
    "\n",
    "# --- Plot 2: Reviews per User (Filtered) ---\n",
    "sns.histplot(\n",
    "    filtered_user_counts,\n",
    "    bins=50,\n",
    "    color=\"#4C72B0\",\n",
    "    edgecolor='white',\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_yscale(\"log\")\n",
    "axes[1].set_title(\"Reviews per User\", fontsize=14)\n",
    "axes[1].set_xlabel(\"Number of Reviews by User\", fontsize=12)\n",
    "axes[1].set_ylabel(\"\")  # Shared y-axis already labeled\n",
    "axes[1].grid(True, linestyle='--', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89316bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot made to resemble the style in the course notes :::\n",
    "# This plot shows us the degree distribution with a comparison to a random graph\n",
    "\n",
    "G_real = G_filtered\n",
    "\n",
    "# --- Step 2: Generate a random graph for comparison\n",
    "n = G_real.number_of_nodes()\n",
    "p = (2 * G_real.number_of_edges()) / (n * (n - 1))  # match average degree\n",
    "G_random = nx.erdos_renyi_graph(n, p)\n",
    "\n",
    "# --- Step 3: Get degree distributions\n",
    "def degree_distribution(graph):\n",
    "    degrees = [d for n, d in graph.degree()]\n",
    "    hist = np.bincount(degrees)\n",
    "    pk = hist / sum(hist)\n",
    "    return np.nonzero(pk)[0], pk[pk > 0]  # degrees, probabilities\n",
    "\n",
    "deg_real, pk_real = degree_distribution(G_real)\n",
    "deg_rand, pk_rand = degree_distribution(G_random)\n",
    "\n",
    "# --- Step 4: Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.loglog(deg_rand, pk_rand, 'o-', color='red', label=\"Random Network\")\n",
    "plt.loglog(deg_real, pk_real, 'o-', color='blue', label=\"Amazon Network\")\n",
    "\n",
    "# --- Step 5: Annotate with average/median degree\n",
    "avg_deg_real = np.mean([d for _, d in G_real.degree()])\n",
    "med_deg_real = np.median([d for _, d in G_real.degree()])\n",
    "avg_deg_rand = np.mean([d for _, d in G_random.degree()])\n",
    "\n",
    "# Add vertical lines\n",
    "plt.axvline(avg_deg_rand, linestyle=\"--\", color=\"yellow\", label=f\"Avg Degree (Random): {avg_deg_rand:.2f}\")\n",
    "plt.axvline(avg_deg_real, linestyle=\"--\", color=\"teal\", label=f\"Avg Degree (Amazon Network): {avg_deg_real:.2f}\")\n",
    "plt.axvline(med_deg_real, linestyle=\"--\", color=\"black\", label=f\"Median Degree (Amazon Network): {med_deg_real:.0f}\")\n",
    "\n",
    "# Labels and legend\n",
    "plt.xlabel(\"Degree k (node)\")\n",
    "plt.ylabel(\"Probability p(k) of degree k\")\n",
    "plt.title(\"Degree Distribution of Amazon Reviews Network vs. Random Network\")\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"both\", linestyle='--', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad88c1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code morphed from previous assignments:\n",
    "# Calculating the clustering coefficients\n",
    "# This shows clear evidence of the small world property!\n",
    "\n",
    "# Step 1: Extract giant component of real network\n",
    "largest_cc = max(nx.connected_components(G_filtered), key=len)\n",
    "G_giant = G_filtered.subgraph(largest_cc).copy()\n",
    "\n",
    "# Step 2: Generate a random network with same size and edge count\n",
    "n = G_giant.number_of_nodes()\n",
    "m = G_giant.number_of_edges()\n",
    "p = (2 * m) / (n * (n - 1))\n",
    "G_random = nx.erdos_renyi_graph(n, p)\n",
    "\n",
    "# Step 3: Approximate average path length (sampling)\n",
    "# Adapted from assignment to speed up computation\n",
    "def approximate_avg_path_length(graph, samples=10000):\n",
    "    nodes = list(graph.nodes())\n",
    "    total_length = 0\n",
    "    valid_samples = 0\n",
    "    for _ in range(samples):\n",
    "        u, v = np.random.choice(nodes, 2, replace=False)\n",
    "        try:\n",
    "            length = nx.shortest_path_length(graph, source=u, target=v)\n",
    "            total_length += length\n",
    "            valid_samples += 1\n",
    "        except nx.NetworkXNoPath:\n",
    "            continue\n",
    "    return total_length / valid_samples if valid_samples > 0 else float('inf')\n",
    "\n",
    "# Step 4: Compute metrics\n",
    "real_path = approximate_avg_path_length(G_giant)\n",
    "rand_path = approximate_avg_path_length(G_random)\n",
    "real_clustering = nx.average_clustering(G_giant)\n",
    "rand_clustering = nx.average_clustering(G_random)\n",
    "\n",
    "# Step 5: Print comparison\n",
    "print(\"=== Small-World Property Comparison ===\")\n",
    "print(f\"Approx. Avg Path Length (Amazon Network): {real_path:.4f}\")\n",
    "print(f\"Approx. Avg Path Length (Random Network): {rand_path:.4f}\")\n",
    "print()\n",
    "print(f\"Clustering Coefficient (Amazon Network):  {real_clustering:.4f}\")\n",
    "print(f\"Clustering Coefficient (Random Network):  {rand_clustering:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740f4624",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = G_filtered.number_of_nodes()\n",
    "m = G_filtered.number_of_edges()\n",
    "\n",
    "p_actual = (2 * m) / (n * (n - 1))\n",
    "p_critical = 1 / n\n",
    "p_connected = np.log(n) / n\n",
    "\n",
    "print(\"=== Connectivity Analysis ===\")\n",
    "print(f\"Number of nodes (n):             {n}\")\n",
    "print(f\"Number of edges (m):             {m}\")\n",
    "print(f\"Actual edge probability (p):     {p_actual:.6f}\")\n",
    "print(f\"Critical threshold (1/n):        {p_critical:.6f}\")\n",
    "print(f\"Connected threshold (log(n)/n):  {p_connected:.6f}\")\n",
    "print()\n",
    "print(f\"Giant Component Exists?          {p_actual > p_critical}\")\n",
    "print(f\"Graph Likely Fully Connected?    {p_actual > p_connected}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299797b0",
   "metadata": {},
   "source": [
    "Supporting explanation is done on the website. To be more concrete we've used the network theory with the following formulas:\n",
    "\n",
    "A small-world network is one that has:\n",
    "- **High clustering** (nodes form tight communities)\n",
    "- **Short average path lengths** (any two nodes can be reached in a small number of steps)\n",
    "\n",
    "The clustering coefficient is given:\n",
    "\n",
    "$$\n",
    "C = \\frac{1}{n} \\sum_{v \\in V} \\frac{2e_v}{k_v(k_v - 1)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( k_v \\): degree of node \\( v \\)\n",
    "- \\( e_v \\): number of edges between neighbors of \\( v \\)\n",
    "\n",
    "We compute the edge probability as:\n",
    "\n",
    "$$\n",
    "p = \\frac{2m}{n(n - 1)}\n",
    "$$\n",
    "\n",
    "And compare it to the two ER thresholds:\n",
    "- **Giant component threshold**:  \n",
    "  $$\n",
    "  p_{\\text{critical}} = \\frac{1}{n}\n",
    "  $$\n",
    "\n",
    "- **Connectedness threshold**:  \n",
    "  $$\n",
    "  p_{\\text{connected}} = \\frac{\\log n}{n}\n",
    "  $$\n",
    "\n",
    "To summarize, it's clear that the network falls into the \"supercritical\" regime. There is clear evidence of the small world property, as is expected with natural networks. Additionally, we demonstrate how it differs from the random network that lacks the same clustering coefficient. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8f8cc9",
   "metadata": {},
   "source": [
    "# Community Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e683d4f5",
   "metadata": {},
   "source": [
    "In the first part, we are using the Louvain algorithm to find graph communities. We will filter for communities larger thatn 100 nodes as our baseline, then look more closely at some of the more interesting ones. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24324d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"network_filtered.json\", \"r\") as f:\n",
    "    graph_data = json.load(f)\n",
    "\n",
    "Graph = json_graph.node_link_graph(graph_data)\n",
    "\n",
    "print(f\"Filtered Graph: {Graph.number_of_nodes()} nodes, {Graph.number_of_edges()} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b619f3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using random state 100, we've also set np.random to state 100. \n",
    "# In spite of this we've noticed that the partitioning may change when the code is run\n",
    "# multiple times. You may get slightly different results as compared to what we have\n",
    "# in the project. \n",
    "\n",
    "partition = community_louvain.best_partition(Graph, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136b8c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets filter away tiny communities and put it into neat dictionaries for easier access:\n",
    "\n",
    "communities = defaultdict(list)\n",
    "for node, comm_id in partition.items():\n",
    "    communities[comm_id].append(node)\n",
    "\n",
    "communities = defaultdict(list, {\n",
    "    k: v for k, v in communities.items() if len(v) > 100\n",
    "})\n",
    "\n",
    "for comm_id, members in communities.items():\n",
    "    print(f\"Community {comm_id}: {members}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5b1d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Community sizes + ID's\n",
    "def print_community_sizes(community_dict):\n",
    "    for community_id, asin_list in community_dict.items():\n",
    "        print(f\"Community {community_id}: {len(asin_list)} items\")\n",
    "\n",
    "print_community_sizes(communities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1658f190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to json so other notebooks can open it in a convenient way:\n",
    "with open('communities.json', 'w') as file:\n",
    "    json.dump(communities, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7288865",
   "metadata": {},
   "source": [
    "Below are a range of functions in order to preprocess and clean up the dataset for analysis. In the first step we will show how we extract a genre whitelist from the categories column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac40a95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function parses the categories column, we will use this later to construct a whitelist\n",
    "#of terms to catch genres:\n",
    "def extract_unique_category_terms(df, column='categories'):\n",
    "    all_terms = set()\n",
    "    \n",
    "    for raw_entry in df[column].dropna():\n",
    "        try:\n",
    "            # If the entry is stringified like a list, we safely convert it back to a list\n",
    "            parsed = eval(raw_entry)\n",
    "            if isinstance(parsed, list):\n",
    "                all_terms.update(parsed)\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing entry: {raw_entry}, Error: {e}\")\n",
    "    \n",
    "    return all_terms\n",
    "\n",
    "all_category_terms = extract_unique_category_terms(df) #Every unique term in categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e216c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will print these. After printing we insert every word into an LLM\n",
    "# To parse the valid genre names and cut through the noise. \n",
    "# In addition - manual overview and selection has been done to get the final whitelist\n",
    "print(all_category_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee5dbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function to speed up the next code block\n",
    "def build_asin_to_categories(df):\n",
    "    \"\"\"Preprocess the dataframe to create a fast lookup dictionary.\"\"\"\n",
    "    asin_to_categories = {}\n",
    "    for _, row in df.iterrows():\n",
    "        asin = row['parent_asin']\n",
    "        try:\n",
    "            categories = ast.literal_eval(row['categories']) if pd.notna(row['categories']) else []\n",
    "        except (ValueError, SyntaxError, TypeError):\n",
    "            categories = []\n",
    "        asin_to_categories[asin] = categories\n",
    "    return asin_to_categories\n",
    "\n",
    "asin_to_categories = build_asin_to_categories(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94eade5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using an LLM to parse for genre-like terms + manual curation to get the list:\n",
    "valid_genres = [\n",
    "    'action', 'adventure', 'comedy', 'drama', 'fantasy', 'sci-fi', 'science fiction',\n",
    "    'horror', 'thriller', 'romance', 'mystery', 'animation', 'documentary', 'family', \n",
    "    'musicals', 'biography', 'historical', 'crime', 'music', 'war', 'western', 'kids', \n",
    "    'educational', 'reality', 'anime', 'fitness', 'yoga', 'exercise', 'sports', \n",
    "]\n",
    "\n",
    "# This function counts the number of matches from valid_genres in the categories column\n",
    "# for a certain community:\n",
    "def get_top_genres_for_community(community_asins, asin_to_categories, top_n):\n",
    "    genre_counter = Counter()\n",
    "\n",
    "    for asin in community_asins:\n",
    "        categories = asin_to_categories.get(asin, [])\n",
    "        for cat in categories:\n",
    "            cat_lower = cat.lower()\n",
    "            for genre in valid_genres:\n",
    "                if genre in cat_lower:\n",
    "                    genre_counter[genre] += 1\n",
    "\n",
    "    return dict(genre_counter.most_common(top_n))\n",
    "\n",
    "# Try it out here by putting in a community id:\n",
    "# We can also use this code to get some future plots of the actual statistics\n",
    "# If that makes sense, like make a for loop for each id.\n",
    "community_id = 1\n",
    "asin_list = communities[community_id]\n",
    "top_genres = get_top_genres_for_community(asin_list, asin_to_categories, 10)\n",
    "print(top_genres)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e25e67a",
   "metadata": {},
   "source": [
    "With the functions above, we now have a framework to count genre frequencies in each community. This provides a dictionary with genre + frequency for every match in the whitelist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0155e86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also need a function to extract images for the plotting function:\n",
    "\n",
    "# Load the metadata dataset that contains image URLs\n",
    "meta_df = pd.read_csv('meta_Movies_and_TV_Full.csv')\n",
    "\n",
    "asin_to_image = {}\n",
    "\n",
    "for _, row in meta_df.iterrows():\n",
    "    asin = row['parent_asin']\n",
    "    image_data = row['images']\n",
    "    \n",
    "    # Check if the image_data is a list of dictionaries\n",
    "    if isinstance(image_data, str):\n",
    "        try:\n",
    "            image_urls = eval(image_data)\n",
    "            for image in image_urls:\n",
    "                if isinstance(image, dict):  # Check if it's a dictionary\n",
    "                    if 'large' in image:\n",
    "                        asin_to_image[asin] = image['large']\n",
    "                        break\n",
    "                    elif '360w' in image:\n",
    "                        asin_to_image[asin] = 'no image' #These images aren't very good so left out\n",
    "                        break\n",
    "        except (ValueError, SyntaxError, TypeError):\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e493958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can grab the image URL for any ASIN here, only works for \"Movies\", Prime Video \n",
    "# Doesn't have proper images but random stills from the movie, left out for now.\n",
    "# You can feel free to change it in the above function and see how the plots\n",
    "# Change in the code below. Or you can test it right here:\n",
    "\n",
    "asin_example = 'B001JV5BF8'\n",
    "image_url = asin_to_image.get(asin_example, 'Invalid Key')\n",
    "print(f\"Image URL for ASIN {asin_example}: {image_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc668e9",
   "metadata": {},
   "source": [
    "Here comes the first major roadblock: The movie titles themselves are not standardized in the dataset. Meaning, different versions of the same movie come up with different formats (Blu Ray/DVD/Digital), and there's the issue of boxed sets, combo formats, and sequels with otherwise identical titles. So, in order to get a standardized baseline to use when selecting the top movie picks in each community we need to use regular expressions to try and filter out all the extraneous noise. \n",
    "\n",
    "When we plot the titles themselves we will use the \"real\" titles. But at the comparison stage regex are used to avoid duplicate movies coming up in the same ranking. It will just pick the top movie and then reject it's regex'ed duplicate if it's found in the same community. This just gives a nicer overview.\n",
    "\n",
    "For comparison we remove:\n",
    " * [DVD], [Blu-ray] etc.\n",
    " * (Special Edition) etc.\n",
    " * Strip out all punctuation\n",
    " * Remove whitespace around leading/trailing movie titles\n",
    "\n",
    "You can look at the specific functionality in the code just below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e554fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code uses regular expressions to filter out titles in order to avoid duplicates\n",
    "# In the get_top_titles functions:\n",
    "def normalize_title(title):\n",
    "    if not isinstance(title, str):\n",
    "        return ''\n",
    "    title = title.lower()\n",
    "    \n",
    "    # Keep part after comma if it exists and the part after is short (likely the title)\n",
    "    if ',' in title:\n",
    "        parts = title.split(',')\n",
    "        if len(parts[1].strip().split()) <= 4:\n",
    "            title = parts[1]\n",
    "    \n",
    "    title = re.sub(r'\\[.*?\\]', '', title)              # remove [DVD], [Blu-ray], etc.\n",
    "    title = re.sub(r'\\(.*?edition\\)', '', title)       # remove (Special Edition), etc.\n",
    "    title = re.sub(r'\\(.*?format\\)', '', title)        # remove other bracketed formats\n",
    "    title = re.sub(r'[^a-z0-9\\s]', '', title)          # remove punctuation\n",
    "    title = re.sub(r'\\s+', ' ', title).strip()         # normalize whitespace\n",
    "    return title\n",
    "\n",
    "# Simply returns the top titles, based on the rating_number WITHOUT the genre filter:\n",
    "# Probably not going to use this function much, but it's nice to have.\n",
    "def get_top_titles_for_community(community_asins, df, top_n=10):\n",
    "    df['rating_number'] = df['rating_number'].apply(pd.to_numeric, errors='coerce')\n",
    "    matching_rows = df[df['parent_asin'].isin(community_asins)]\n",
    "    \n",
    "    deduped = matching_rows.sort_values('rating_number', ascending=False).drop_duplicates('parent_asin')\n",
    "    \n",
    "    top_rows = deduped.sort_values('rating_number', ascending=False)\n",
    "    top_rows = top_rows[top_rows['movie_title'].notna()]\n",
    "    \n",
    "    seen = set()\n",
    "    unique_top_titles = []\n",
    "    \n",
    "    for _, row in top_rows.iterrows():\n",
    "        norm_title = normalize_title(row['movie_title'])\n",
    "        if norm_title not in seen:\n",
    "            seen.add(norm_title)\n",
    "            unique_top_titles.append((row['movie_title'], row['rating_number']))\n",
    "        if len(unique_top_titles) >= top_n:\n",
    "            break\n",
    "\n",
    "    return unique_top_titles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57c7cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function without filter here:\n",
    "community_id = 44\n",
    "asin_list = communities[community_id]\n",
    "top_titles = get_top_titles_for_community(asin_list, df)\n",
    "for t in top_titles:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499d05f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block filters the top tiles WITH the top genre filter to get a better\n",
    "# overview of the type of movie in a community. Why? Because the communities generally\n",
    "# Share genres like action/drama so the same types of movies will otherwise dominate\n",
    "# in every communities top hits.\n",
    "\n",
    "def get_top_titles_by_genre_for_community(community_asins, df, asin_to_categories, top_n=10):\n",
    "    # First get the top genre for this community\n",
    "    top_genres = get_top_genres_for_community(community_asins, asin_to_categories, 1)\n",
    "    top_genre = next(iter(top_genres.keys()))\n",
    "    \n",
    "    # Get matching rows for this community\n",
    "    matching_rows = df[df['parent_asin'].isin(community_asins)]\n",
    "    \n",
    "    # Filter rows that have the top genre in their categories\n",
    "    filtered_rows = []\n",
    "    for _, row in matching_rows.iterrows():\n",
    "        categories = asin_to_categories.get(row['parent_asin'], [])\n",
    "        categories_lower = [cat.lower() for cat in categories]\n",
    "        if any(top_genre in cat for cat in categories_lower):\n",
    "            filtered_rows.append(row)\n",
    "    \n",
    "    # Create a DataFrame from filtered rows\n",
    "    filtered_df = pd.DataFrame(filtered_rows)\n",
    "    \n",
    "    # Deduplicate and sort by rating_number\n",
    "    deduped = filtered_df.sort_values('rating_number', ascending=False).drop_duplicates('parent_asin')\n",
    "    top_rows = deduped.sort_values('rating_number', ascending=False)\n",
    "    top_rows = top_rows[top_rows['movie_title'].notna()]\n",
    "    \n",
    "    # Get unique titles\n",
    "    seen = set()\n",
    "    unique_top_titles = []\n",
    "    \n",
    "    for _, row in top_rows.iterrows():\n",
    "        norm_title = normalize_title(row['movie_title'])\n",
    "        if norm_title not in seen:\n",
    "            seen.add(norm_title)\n",
    "            unique_top_titles.append((row['movie_title'], row['rating_number']))\n",
    "        if len(unique_top_titles) >= top_n:\n",
    "            break\n",
    "\n",
    "    return unique_top_titles, top_genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce847a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you can test the genre filtered version:\n",
    "# This is the one we use in the plotting function below as well\n",
    "# And the substrate to the project analysis.\n",
    "community_id = 44\n",
    "asin_list = communities[community_id]\n",
    "top_titles, top_genre = get_top_titles_by_genre_for_community(asin_list, df, asin_to_categories, 20)\n",
    "\n",
    "print(f\"Top {top_genre} movies in community {community_id}:\")\n",
    "for title, rating_count in top_titles:\n",
    "    print(f\"  > {title} (Ratings: {rating_count})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ca4029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_community_summary(\n",
    "    comm_id,\n",
    "    communities,\n",
    "    df_full,\n",
    "    asin_to_image,\n",
    "    get_top_titles_by_genre_for_community,\n",
    "    get_top_genres_for_community,\n",
    "    asin_to_categories,\n",
    "    top_n_display=9,\n",
    "    buffer_size=1000,\n",
    "    top_n_genres=6\n",
    "):\n",
    "    community_asins = communities[comm_id]\n",
    "\n",
    "    top_title_tuples, top_genre = get_top_titles_by_genre_for_community(\n",
    "        community_asins=community_asins,\n",
    "        df=df_full,\n",
    "        asin_to_categories=asin_to_categories,\n",
    "        top_n=buffer_size\n",
    "    )\n",
    "    top_titles = [t[0] for t in top_title_tuples]\n",
    "    title_to_count = dict(top_title_tuples)\n",
    "\n",
    "    images, titles, counts = [], [], []\n",
    "    for title in top_titles:\n",
    "        if len(images) == top_n_display:\n",
    "            break\n",
    "        row = df_full[df_full['movie_title'].str.lower() == title.lower()]\n",
    "        if row.empty: continue\n",
    "        asin = row.iloc[0]['asin']\n",
    "        url = asin_to_image.get(asin)\n",
    "        if not url or url == \"no image\": continue\n",
    "        resp = requests.get(url, timeout=5)\n",
    "        img = Image.open(BytesIO(resp.content)).resize((220, 330))\n",
    "        images.append(img)\n",
    "        titles.append(title)\n",
    "        counts.append(title_to_count.get(title, 0))\n",
    "\n",
    "    genre_dict = get_top_genres_for_community(\n",
    "        community_asins,\n",
    "        asin_to_categories,\n",
    "        top_n_genres\n",
    "    )\n",
    "    top_genres = sorted(genre_dict.items(), key=lambda x: -x[1])\n",
    "\n",
    "    # Plot setup\n",
    "    fig = plt.figure(figsize=(10.5, 6))\n",
    "    gs = fig.add_gridspec(3, 5, width_ratios=[1, 1, 1, 0.4, 1.2], wspace=0.05, hspace=0.3)\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        ax = fig.add_subplot(gs[i // 3, i % 3])\n",
    "        ax.imshow(img)\n",
    "        label = f\"{titles[i].capitalize()[:22]}\\n({int(counts[i]):,} ratings)\"\n",
    "        ax.set_title(label, fontsize=8, pad=8) \n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    axg = fig.add_subplot(gs[:, 4])\n",
    "    if top_genres:\n",
    "        labels, vals = zip(*top_genres)\n",
    "        labels = [g.capitalize() for g in labels]\n",
    "        axg.barh(labels[::-1], vals[::-1], color='skyblue')\n",
    "        axg.set_xlabel(\"Frequency\")\n",
    "        axg.set_title(\"Top Genres\")\n",
    "\n",
    "    fig.suptitle(f\"Community {comm_id} â€” {top_genre.capitalize()}\", fontsize=14)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe562afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available ID's\n",
    "# [44, 1, 2, 3, 5, 147, 8, 9, 10, 11, 13, 17, 37]\n",
    "\n",
    "# Simply insert the ID here (after running all the above code) to test:\n",
    "# This code ONLY picks the \"Physical Movie\" type, since the streaming/digital titles\n",
    "# Don't have a proper product preview. This means some highly popular titles\n",
    "# Wont show up here, even though they are present in the community.\n",
    "plot_community_summary(\n",
    "    comm_id=2,\n",
    "    communities=communities,\n",
    "    df_full=df,\n",
    "    asin_to_image=asin_to_image,\n",
    "    get_top_titles_by_genre_for_community=get_top_titles_by_genre_for_community,\n",
    "    get_top_genres_for_community=get_top_genres_for_community,\n",
    "    asin_to_categories=asin_to_categories\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e31742f",
   "metadata": {},
   "source": [
    "### Assortativity\n",
    "\n",
    "In this section we will go over the use of assortativity. With assortativity you can track how nodes in your network tend to gather with other networks of some similar feature. It's possible to look at degree assortativity for instance. Here we are chosing dominant genre assortativity. This measures how dominant the primary genre is, within a community. Using this we get a numeric tool to support each community. We also experimented with sentiment based assortativity but found just noisy patterns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6c0a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df is just the merged dataset with genres and stuff\n",
    "def extract_primary_genre(categories, valid_genres):\n",
    "    for cat in categories:\n",
    "        cat_lower = cat.lower()\n",
    "        for genre in valid_genres:\n",
    "            if genre in cat_lower:\n",
    "                return genre\n",
    "    return None  # no valid genre matched\n",
    "\n",
    "asin_to_genre = {\n",
    "    asin: extract_primary_genre(categories, valid_genres)\n",
    "    for asin, categories in asin_to_categories.items()\n",
    "    if extract_primary_genre(categories, valid_genres) is not None\n",
    "}\n",
    "\n",
    "def compute_genre_assortativity(G, asin_list, asin_to_genre):\n",
    "    valid_asins = [asin for asin in asin_list if asin in asin_to_genre]\n",
    "    if len(valid_asins) < 2:\n",
    "        return None\n",
    "\n",
    "    subG = G.subgraph(valid_asins).copy()\n",
    "\n",
    "    for asin in subG.nodes:\n",
    "        subG.nodes[asin]['genre'] = asin_to_genre[asin]\n",
    "\n",
    "    return nx.attribute_assortativity_coefficient(subG, 'genre')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d65bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "for comm_id in (communities.keys()):\n",
    "    score = compute_genre_assortativity(Graph, communities[comm_id], asin_to_genre)\n",
    "    if score is not None:\n",
    "        print(f\"Genre assortativity for community {comm_id}: {score:.4f}\")\n",
    "    else:\n",
    "        print(f\"Genre assortativity for community {comm_id}: not enough data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a94350",
   "metadata": {},
   "source": [
    "### Centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1c197b",
   "metadata": {},
   "source": [
    "Centrality measures can tell us about the globally influential nodes. Due to our community driven analysis we wanted to look at eigen centrality. Eigen centrality looks at influence based on how well connected the node is, the number of links, and how it compares to neighbouring nodes. It will pick out movies from highly connected regions of the graph that has massive influence in particular places.\n",
    "\n",
    "Closeness is a broad score that tracks how close the node is to every other node in the network. It computes the shortest paths between the other nodes and ranks them. This measure is particularly useful to find the globally important movies that has influence on the entire network. We wanted to look at these to test the hypothesis that a few hyper ppopular movies are indeed dominating the graph. Which turns out to be the case.\n",
    "\n",
    "On the project website we've futher specified what this means concretely for the purposes of a reccomendation algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b93b78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract a subgraph with just the titled movies. \n",
    "valid_asins = df.dropna(subset=['movie_title'])['parent_asin'].unique().tolist()\n",
    "G_with_titles = Graph.subgraph(valid_asins).copy()\n",
    "print(f\"Filtered Graph: {G_with_titles.number_of_nodes()} nodes, {G_with_titles.number_of_edges()} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369faaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are using the built in networkX functions:\n",
    "closeness = nx.closeness_centrality(G_with_titles)\n",
    "eigen = nx.eigenvector_centrality(G_with_titles, max_iter=1000, tol=1e-06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912aa399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets get the top 10 movies:\n",
    "def print_top_central_movies(centrality_dict, df, top_n=10, label=\"Centrality\"):\n",
    "    top_nodes = sorted(centrality_dict.items(), key=lambda x: -x[1])[:top_n]\n",
    "\n",
    "    for asin, score in top_nodes:\n",
    "        row = df[(df['parent_asin'] == asin) | (df['asin'] == asin)]\n",
    "        if not row.empty and pd.notna(row.iloc[0]['movie_title']):\n",
    "            title = row.iloc[0]['movie_title']\n",
    "            print(f\"{title} ({asin}) â€” {label}: {score:.4f}\")\n",
    "\n",
    "print()        \n",
    "print(\"Closeness Scores\")\n",
    "print()\n",
    "print_top_central_movies(closeness, df, top_n=10, label=\"Closeness\")\n",
    "\n",
    "print()        \n",
    "print(\"Eigen Scores\")\n",
    "print()\n",
    "print_top_central_movies(eigen, df, top_n=10, label=\"Eigenvector\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
